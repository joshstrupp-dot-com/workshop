{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bdh8ivekoIFs",
      "metadata": {
        "id": "bdh8ivekoIFs"
      },
      "source": [
        "# SDXL + DreamBooth + LoRA – Local Training Example\n",
        "\n",
        "In this notebook, we demonstrate how to fine-tune Stable Diffusion XL (SDXL) with DreamBooth\n",
        "using LoRA (Low-Rank Adaptation) for local usage.  \n",
        "\n",
        "\n",
        "LoRA works by injecting low-rank adaptation matrices into certain layers of a large model,\n",
        "significantly reducing the number of trainable parameters. This method enables the model\n",
        "to efficiently adapt to new concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cusLqhqqoZWR",
      "metadata": {
        "id": "cusLqhqqoZWR"
      },
      "source": [
        "## 1. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "r3Zm3MnVoIFv",
      "metadata": {
        "id": "r3Zm3MnVoIFv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m--2025-02-19 11:28:55--  https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84922 (83K) [text/plain]\n",
            "Saving to: ‘train_dreambooth_lora_sdxl.py’\n",
            "\n",
            "train_dreambooth_lo 100%[===================>]  82.93K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-19 11:28:55 (3.84 MB/s) - ‘train_dreambooth_lora_sdxl.py’ saved [84922/84922]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "# !nvidia-smi\n",
        "\n",
        "# Install dependencies\n",
        "!pip install bitsandbytes transformers accelerate peft -q\n",
        "!pip install git+https://github.com/huggingface/diffusers.git -q\n",
        "\n",
        "# Download the DreamBooth + LoRA SDXL training script\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DTY4ph2MoIFw",
      "metadata": {
        "id": "DTY4ph2MoIFw"
      },
      "source": [
        "## 2. Dataset\n",
        "Below we show how to either upload images locally or download example data from the Hugging Face Hub.\n",
        "Make sure you have your training images in a local folder. You can also auto-generate captions\n",
        "using a BLIP model if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-_XReAK0oIFw",
      "metadata": {
        "id": "-_XReAK0oIFw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "local_dir = \"./dog/\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "os.chdir(local_dir)\n",
        "\n",
        "# Uncomment to upload images manually:\n",
        "# uploaded_images = files.upload()\n",
        "\n",
        "# Comment if using uploaded images:\n",
        "os.chdir(\"/content\")\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\n",
        "    \"diffusers/dog-example\",\n",
        "    local_dir=local_dir,\n",
        "    repo_type=\"dataset\",\n",
        "    ignore_patterns=\".gitattributes\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols, resize=256):\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "img_paths = \"./dog/*.jpeg\"\n",
        "imgs = [Image.open(path) for path in glob.glob(img_paths)]\n",
        "num_imgs_to_preview = min(5, len(imgs))\n",
        "display(image_grid(imgs[:num_imgs_to_preview], 1, num_imgs_to_preview))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTyDeannoIFx",
      "metadata": {
        "id": "UTyDeannoIFx"
      },
      "source": [
        "## 3. Auto-generate Captions with BLIP\n",
        "You can generate image captions automatically, then prepend or append with tokens relevant to your concept.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gOZuacZkoIFx",
      "metadata": {
        "id": "gOZuacZkoIFx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "import glob\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "\n",
        "def caption_images(input_image):\n",
        "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    pixel_values = inputs.pixel_values\n",
        "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption\n",
        "\n",
        "\n",
        "# Specify folder containing images to be tagged:\n",
        "local_dir = \"./dog/\"\n",
        "imgs_and_paths = [(path, Image.open(path)) for path in glob.glob(f\"{local_dir}*.jpeg\")]\n",
        "\n",
        "# Add desired captioning prefix to each image:\n",
        "caption_prefix = \"a photo of TOK dog, \"\n",
        "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
        "    for (path, img) in imgs_and_paths:\n",
        "        caption = caption_prefix + caption_images(img).split(\"\\n\")[0]\n",
        "        entry = {\"file_name\": path.split(\"/\")[-1], \"prompt\": caption}\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "\n",
        "# Clean up memory\n",
        "del blip_processor, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IA0IBrB5oIFx",
      "metadata": {
        "id": "IA0IBrB5oIFx"
      },
      "source": [
        "## 4. Prepare Accelerate & Configuration\n",
        "Initialize an Accelerate config, which helps handle multi-GPU or single-GPU setups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SS3QjPTBoIFy",
      "metadata": {
        "id": "SS3QjPTBoIFy"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h9LlUMlHoIFy",
      "metadata": {
        "id": "h9LlUMlHoIFy"
      },
      "source": [
        "## 5. Train the Model\n",
        "We call the training script with relevant parameters, including LoRA settings for DreamBooth.\n",
        "This saves LoRA weights to a local directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lIkgAkE1oIFy",
      "metadata": {
        "id": "lIkgAkE1oIFy"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q\n",
        "\n",
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"dog\" \\\n",
        "  --output_dir=\"corgy_dog_LoRA\" \\\n",
        "  --caption_column=\"prompt\" \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --instance_prompt=\"a photo of TOK dog\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=500 \\\n",
        "  --checkpointing_steps=717 \\\n",
        "  --seed=\"0\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1mGFaENmoIFy",
      "metadata": {
        "id": "1mGFaENmoIFy"
      },
      "source": [
        "## 6. Local Inference\n",
        "Once training has finished, we have a local folder (e.g., \"corgy_dog_LoRA\") containing LoRA weights.\n",
        "We load them into the SDXL pipeline to generate new images for our concept:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xaQ6zP-AoIFy",
      "metadata": {
        "id": "xaQ6zP-AoIFy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "\n",
        "lora_folder = \"corgy_dog_LoRA\"\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Load local LoRA weights\n",
        "pipe.load_lora_weights(lora_folder)\n",
        "\n",
        "prompt = \"a photo of TOK dog in a new york\"\n",
        "image = pipe(prompt=prompt, num_inference_steps=25).images[0]\n",
        "display(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "apsA7ugKoIFz",
      "metadata": {
        "id": "apsA7ugKoIFz"
      },
      "source": [
        "### References\n",
        "\n",
        "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
        "- [DreamBooth Paper](https://arxiv.org/abs/2208.12242)\n",
        "- [Huggingface Diffusers Github](https://github.com/huggingface/diffusers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
